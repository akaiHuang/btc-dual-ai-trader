#!/usr/bin/env python3
"""
å®Œæ•´å¤§å–®æ•¸æ“šä¸‹è¼‰å·¥å…· (2020-2025)

ä¸‹è¼‰ Binance BTCUSDT æ­·å² Aggregate Trades æ•¸æ“š
åªä¿ç•™å¤§æ–¼é–¾å€¼çš„å¤§å–® (é è¨­ >=10 BTC)

ä½¿ç”¨æ–¹æ³•:
    python scripts/download_agg_trades_full.py --start 2020-01-01 --end 2025-11-15
    
ç‰¹é»:
    1. åˆ†æ‰¹ä¸‹è¼‰é¿å… API é™åˆ¶
    2. è‡ªå‹•é‡è©¦æ©Ÿåˆ¶
    3. æ–·é»çºŒå‚³ï¼ˆé¿å…é‡è¤‡ä¸‹è¼‰ï¼‰
    4. å¯¦æ™‚é€²åº¦é¡¯ç¤º
    5. è‡ªå‹•åˆä½µåˆ° 15m Kç·š
"""

import pandas as pd
import requests
import time
from datetime import datetime, timedelta
import argparse
import os
import json
from pathlib import Path
from tqdm import tqdm


class AggTradesDownloader:
    """å®Œæ•´å¤§å–®æ•¸æ“šä¸‹è¼‰å™¨"""
    
    def __init__(
        self,
        symbol: str = "BTCUSDT",
        min_qty: float = 10.0,
        output_dir: str = "data/historical",
        batch_size_hours: int = 24,  # æ¯æ¬¡ä¸‹è¼‰ 24 å°æ™‚
        max_retries: int = 3,
        retry_delay: int = 5
    ):
        self.symbol = symbol
        self.min_qty = min_qty
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.batch_size_hours = batch_size_hours
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        
        # Binance API endpoints
        self.base_url = "https://api.binance.com"
        self.agg_trades_endpoint = f"{self.base_url}/api/v3/aggTrades"
        
        # è‡¨æ™‚æ–‡ä»¶è·¯å¾‘
        self.temp_file = self.output_dir / f"{symbol}_agg_trades_temp.parquet"
        self.progress_file = self.output_dir / f"{symbol}_download_progress.json"
        
    def _load_progress(self):
        """è¼‰å…¥ä¸‹è¼‰é€²åº¦"""
        if self.progress_file.exists():
            with open(self.progress_file, 'r') as f:
                return json.load(f)
        return {'completed_batches': [], 'last_timestamp': None}
    
    def _save_progress(self, progress):
        """ä¿å­˜ä¸‹è¼‰é€²åº¦"""
        with open(self.progress_file, 'w') as f:
            json.dump(progress, f, indent=2)
    
    def _timestamp_to_ms(self, dt: datetime) -> int:
        """datetime è½‰æ¯«ç§’æ™‚é–“æˆ³"""
        return int(dt.timestamp() * 1000)
    
    def _download_batch(
        self,
        start_time: datetime,
        end_time: datetime,
        retry_count: int = 0
    ) -> pd.DataFrame:
        """
        ä¸‹è¼‰å–®å€‹æ‰¹æ¬¡çš„æ•¸æ“š
        
        Args:
            start_time: é–‹å§‹æ™‚é–“
            end_time: çµæŸæ™‚é–“
            retry_count: é‡è©¦æ¬¡æ•¸
            
        Returns:
            DataFrame with columns: [timestamp, price, qty, side, trade_id]
        """
        start_ms = self._timestamp_to_ms(start_time)
        end_ms = self._timestamp_to_ms(end_time)
        
        # æ³¨æ„ï¼šBinance API ä¸å…è¨± startTime/endTime å’Œ fromId åŒæ™‚ä½¿ç”¨
        # å› æ­¤æ¯å€‹ 24 å°æ™‚æ‰¹æ¬¡æœ€å¤šåªèƒ½ç²å– 1000 ç­†äº¤æ˜“
        # å¦‚æœæŸå¤©æœ‰è¶…é 1000 ç­†å¤§å–®ï¼Œéœ€è¦ç¸®å°æ‰¹æ¬¡å¤§å°ï¼ˆä¾‹å¦‚æ”¹ç‚º 12 å°æ™‚ï¼‰
        
        params = {
            'symbol': self.symbol,
            'startTime': start_ms,
            'endTime': end_ms,
            'limit': 1000
        }
        
        try:
            response = requests.get(self.agg_trades_endpoint, params=params, timeout=30)
            
            # è™•ç† 429 Too Many Requests
            if response.status_code == 429:
                retry_after = int(response.headers.get('Retry-After', self.retry_delay))
                print(f"âš ï¸  API é™æµï¼Œç­‰å¾… {retry_after} ç§’...")
                time.sleep(retry_after)
                return self._download_batch(start_time, end_time, retry_count)
            
            response.raise_for_status()
            
            trades = response.json()
            
            # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤ç¢¼
            if isinstance(trades, dict) and 'code' in trades:
                if trades['code'] == -1003:  # WAF limit
                    print(f"âš ï¸  è§¸åŠ WAF é™åˆ¶ï¼Œç­‰å¾… {self.retry_delay * 2} ç§’...")
                    time.sleep(self.retry_delay * 2)
                    return self._download_batch(start_time, end_time, retry_count)
                else:
                    print(f"âš ï¸  API éŒ¯èª¤: {trades}")
                    return pd.DataFrame()
            
            if not trades or len(trades) == 0:
                return pd.DataFrame()
            
            all_trades = trades
            
        except requests.exceptions.RequestException as e:
            if retry_count < self.max_retries:
                wait_time = self.retry_delay * (2 ** retry_count)  # Exponential backoff
                print(f"âš ï¸  è«‹æ±‚å¤±æ•—ï¼Œ{wait_time}ç§’å¾Œé‡è©¦ ({retry_count + 1}/{self.max_retries})...")
                time.sleep(wait_time)
                return self._download_batch(start_time, end_time, retry_count + 1)
            else:
                print(f"âŒ ä¸‹è¼‰å¤±æ•—: {start_time} ~ {end_time}")
                print(f"   éŒ¯èª¤: {e}")
                return pd.DataFrame()
        
        if not all_trades:
            return pd.DataFrame()
        
        # è§£ææ•¸æ“š
        df = pd.DataFrame([{
            'timestamp': pd.to_datetime(t['T'], unit='ms', utc=True).tz_localize(None),  # ç§»é™¤æ™‚å€ï¼Œä¿æŒ UTC
            'price': float(t['p']),
            'qty': float(t['q']),
            'side': 'BUY' if t['m'] == False else 'SELL',  # m=false -> buyer is maker
            'trade_id': t['a']
        } for t in all_trades])
        
        # åªä¿ç•™å¤§å–®
        df = df[df['qty'] >= self.min_qty].copy()
        
        return df
    
    def download_range(
        self,
        start_date: str,
        end_date: str,
        resume: bool = True
    ) -> pd.DataFrame:
        """
        ä¸‹è¼‰æŒ‡å®šæ™‚é–“ç¯„åœçš„å¤§å–®æ•¸æ“š
        
        Args:
            start_date: é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: çµæŸæ—¥æœŸ (YYYY-MM-DD)
            resume: æ˜¯å¦æ–·é»çºŒå‚³
            
        Returns:
            å®Œæ•´çš„å¤§å–®æ•¸æ“š DataFrame
        """
        print("="*70)
        print(f"ğŸš€ é–‹å§‹ä¸‹è¼‰ {self.symbol} å¤§å–®æ•¸æ“š (>={self.min_qty} BTC)")
        print("="*70)
        print(f"æ™‚é–“ç¯„åœ: {start_date} ~ {end_date}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.batch_size_hours} å°æ™‚")
        print()
        print("âš ï¸  æ³¨æ„:")
        print("  - Binance aggTrades æ­·å²å¯èƒ½æœ‰é™åˆ¶ï¼ˆé€šå¸¸åªä¿ç•™æœ€è¿‘ 3 å¹´ï¼‰")
        print("  - å¦‚æœ 2020-2021 æ•¸æ“šç¨€å°‘ï¼Œé€™æ˜¯æ­£å¸¸çš„")
        print("  - æœƒè‡ªå‹•è™•ç† API é™æµå’Œé‡è©¦")
        print()
        
        # è§£ææ—¥æœŸ
        start = datetime.strptime(start_date, "%Y-%m-%d")
        end = datetime.strptime(end_date, "%Y-%m-%d")
        
        # è¼‰å…¥é€²åº¦
        progress = self._load_progress() if resume else {'completed_batches': [], 'last_timestamp': None}
        
        # ç”Ÿæˆæ‰¹æ¬¡åˆ—è¡¨
        batches = []
        current = start
        while current < end:
            batch_end = min(current + timedelta(hours=self.batch_size_hours), end)
            batch_id = f"{current.strftime('%Y%m%d_%H')}"
            
            if batch_id not in progress['completed_batches']:
                batches.append((current, batch_end, batch_id))
            
            current = batch_end
        
        if not batches:
            print("âœ… æ‰€æœ‰æ‰¹æ¬¡å·²ä¸‹è¼‰å®Œæˆï¼")
            return self._load_existing_data()
        
        print(f"ğŸ“‹ å¾…ä¸‹è¼‰æ‰¹æ¬¡: {len(batches)}")
        print(f"ğŸ“¦ å·²å®Œæˆæ‰¹æ¬¡: {len(progress['completed_batches'])}")
        print()
        
        # ä¸‹è¼‰æ•¸æ“š
        all_trades = []
        
        for batch_start, batch_end, batch_id in tqdm(batches, desc="ä¸‹è¼‰é€²åº¦"):
            # ä¸‹è¼‰æ‰¹æ¬¡
            df_batch = self._download_batch(batch_start, batch_end)
            
            if not df_batch.empty:
                all_trades.append(df_batch)
                
                # ä¿å­˜è‡¨æ™‚æ•¸æ“š
                if all_trades:
                    df_temp = pd.concat(all_trades, ignore_index=True)
                    df_temp.to_parquet(self.temp_file)
            
            # æ›´æ–°é€²åº¦
            progress['completed_batches'].append(batch_id)
            progress['last_timestamp'] = batch_end.isoformat()
            self._save_progress(progress)
            
            # é¿å… API é™æµ
            time.sleep(0.5)
        
        # åˆä½µæ‰€æœ‰æ•¸æ“š
        if all_trades:
            df_all = pd.concat(all_trades, ignore_index=True)
            df_all = df_all.sort_values('timestamp').drop_duplicates(subset=['trade_id'])
            
            # ä¿å­˜æœ€çµ‚æ•¸æ“š
            output_file = self.output_dir / f"{self.symbol}_agg_trades_{start_date.replace('-', '')}_{end_date.replace('-', '')}.parquet"
            df_all.to_parquet(output_file)
            
            print()
            print("="*70)
            print("âœ… ä¸‹è¼‰å®Œæˆï¼")
            print("="*70)
            print(f"ç¸½å¤§å–®æ•¸: {len(df_all):,} ç­†")
            print(f"æ™‚é–“ç¯„åœ: {df_all['timestamp'].min()} ~ {df_all['timestamp'].max()}")
            print(f"å¹³å‡å–®é‡: {df_all['qty'].mean():.2f} BTC")
            print(f"æœ€å¤§å–®é‡: {df_all['qty'].max():.2f} BTC")
            print(f"è²·å–®æ¯”ä¾‹: {(df_all['side'] == 'BUY').sum() / len(df_all) * 100:.1f}%")
            print(f"ä¿å­˜è·¯å¾‘: {output_file}")
            print()
            
            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            if self.temp_file.exists():
                self.temp_file.unlink()
            if self.progress_file.exists():
                self.progress_file.unlink()
            
            return df_all
        else:
            print("âš ï¸  æœªæ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„å¤§å–®")
            return pd.DataFrame()
    
    def _load_existing_data(self) -> pd.DataFrame:
        """è¼‰å…¥å·²å­˜åœ¨çš„æ•¸æ“š"""
        # å°‹æ‰¾ç¾æœ‰æ–‡ä»¶
        files = list(self.output_dir.glob(f"{self.symbol}_agg_trades_*.parquet"))
        if files:
            print(f"âœ… è¼‰å…¥ç¾æœ‰æ•¸æ“š: {files[0]}")
            return pd.read_parquet(files[0])
        return pd.DataFrame()
    
    def merge_with_klines(self, kline_file: str = None):
        """
        å°‡å¤§å–®æ•¸æ“šåˆä½µåˆ° 15m Kç·š
        
        Args:
            kline_file: Kç·šæ•¸æ“šæ–‡ä»¶è·¯å¾‘ï¼Œé è¨­ç‚º BTCUSDT_15m.parquet
        """
        print()
        print("="*70)
        print("ğŸ“Š åˆä½µå¤§å–®æ•¸æ“šåˆ° 15m Kç·š")
        print("="*70)
        
        # è¼‰å…¥ Kç·šæ•¸æ“š
        if kline_file is None:
            kline_file = self.output_dir / "BTCUSDT_15m.parquet"
        
        if not Path(kline_file).exists():
            print(f"âŒ Kç·šæ–‡ä»¶ä¸å­˜åœ¨: {kline_file}")
            return
        
        df_kline = pd.read_parquet(kline_file)
        df_kline['timestamp'] = pd.to_datetime(df_kline['timestamp'])
        
        # è¼‰å…¥å¤§å–®æ•¸æ“š
        agg_files = list(self.output_dir.glob(f"{self.symbol}_agg_trades_*.parquet"))
        if not agg_files:
            print("âŒ æœªæ‰¾åˆ°å¤§å–®æ•¸æ“šæ–‡ä»¶")
            return
        
        df_agg = pd.read_parquet(agg_files[0])
        df_agg['timestamp'] = pd.to_datetime(df_agg['timestamp'])
        
        # ğŸ”§ é—œéµä¿®å¾©ï¼šç§»é™¤æ™‚å€ä¿¡æ¯ï¼Œç¢ºä¿èˆ‡ Kç·šæ•¸æ“šä¸€è‡´
        if df_agg['timestamp'].dt.tz is not None:
            df_agg['timestamp'] = df_agg['timestamp'].dt.tz_localize(None)
        
        print(f"Kç·šæ•¸æ“š: {len(df_kline)} æ ¹")
        print(f"å¤§å–®æ•¸æ“š: {len(df_agg)} ç­†")
        print()
        
        # å°‡å¤§å–®èšåˆåˆ° 15m Kç·š
        df_agg['timestamp_15m'] = df_agg['timestamp'].dt.floor('15min')
        
        # è¨ˆç®—æ¯æ ¹ Kç·šçš„å¤§å–®çµ±è¨ˆ
        agg_stats = df_agg.groupby('timestamp_15m').agg({
            'qty': ['sum', 'count', 'mean', 'max'],
            'side': lambda x: (x == 'BUY').sum() / len(x) if len(x) > 0 else 0.5
        }).reset_index()
        
        agg_stats.columns = [
            'timestamp',
            'large_trade_volume',  # ç¸½å¤§å–®é‡
            'large_trade_count',   # å¤§å–®æ•¸é‡
            'large_trade_avg',     # å¹³å‡å¤§å–®é‡
            'large_trade_max',     # æœ€å¤§å–®é‡
            'large_trade_buy_ratio' # è²·å–®æ¯”ä¾‹
        ]
        
        # åˆä½µ
        df_merged = df_kline.merge(agg_stats, on='timestamp', how='left')
        
        # å¡«å……æ²’æœ‰å¤§å–®çš„ Kç·š
        df_merged['large_trade_volume'] = df_merged['large_trade_volume'].fillna(0)
        df_merged['large_trade_count'] = df_merged['large_trade_count'].fillna(0)
        df_merged['large_trade_avg'] = df_merged['large_trade_avg'].fillna(0)
        df_merged['large_trade_max'] = df_merged['large_trade_max'].fillna(0)
        df_merged['large_trade_buy_ratio'] = df_merged['large_trade_buy_ratio'].fillna(0.5)
        
        # ä¿å­˜
        output_file = self.output_dir / f"{self.symbol}_15m_with_large_trades.parquet"
        df_merged.to_parquet(output_file)
        
        print("âœ… åˆä½µå®Œæˆï¼")
        print(f"è¼¸å‡ºæ–‡ä»¶: {output_file}")
        print(f"ç¸½ Kç·šæ•¸: {len(df_merged)}")
        print(f"æœ‰å¤§å–®çš„ Kç·š: {(df_merged['large_trade_count'] > 0).sum()}")
        print(f"è¦†è“‹ç‡: {(df_merged['large_trade_count'] > 0).sum() / len(df_merged) * 100:.2f}%")
        print()
        
        # çµ±è¨ˆå„å¹´ä»½è¦†è“‹ç‡
        df_merged['year'] = df_merged['timestamp'].dt.year
        yearly_stats = df_merged.groupby('year').agg({
            'large_trade_count': ['count', lambda x: (x > 0).sum()]
        })
        yearly_stats.columns = ['total_candles', 'candles_with_trades']
        yearly_stats['coverage'] = yearly_stats['candles_with_trades'] / yearly_stats['total_candles'] * 100
        
        print("ğŸ“Š å„å¹´ä»½å¤§å–®è¦†è“‹ç‡:")
        print(yearly_stats)
        print()


def main():
    parser = argparse.ArgumentParser(description="ä¸‹è¼‰ Binance æ­·å²å¤§å–®æ•¸æ“š")
    parser.add_argument(
        "--start",
        type=str,
        required=True,
        help="é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)"
    )
    parser.add_argument(
        "--end",
        type=str,
        required=True,
        help="çµæŸæ—¥æœŸ (YYYY-MM-DD)"
    )
    parser.add_argument(
        "--symbol",
        type=str,
        default="BTCUSDT",
        help="äº¤æ˜“å° (é è¨­: BTCUSDT)"
    )
    parser.add_argument(
        "--min_qty",
        type=float,
        default=10.0,
        help="æœ€å°å–®é‡é–¾å€¼ BTC (é è¨­: 10.0)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="data/historical",
        help="è¼¸å‡ºç›®éŒ„ (é è¨­: data/historical)"
    )
    parser.add_argument(
        "--batch_hours",
        type=int,
        default=24,
        help="æ¯æ‰¹æ¬¡å°æ™‚æ•¸ (é è¨­: 24)"
    )
    parser.add_argument(
        "--no_resume",
        action="store_true",
        help="ä¸ä½¿ç”¨æ–·é»çºŒå‚³ï¼ˆé‡æ–°ä¸‹è¼‰ï¼‰"
    )
    parser.add_argument(
        "--no_merge",
        action="store_true",
        help="ä¸åˆä½µåˆ° Kç·šæ•¸æ“š"
    )
    
    args = parser.parse_args()
    
    # å‰µå»ºä¸‹è¼‰å™¨
    downloader = AggTradesDownloader(
        symbol=args.symbol,
        min_qty=args.min_qty,
        output_dir=args.output_dir,
        batch_size_hours=args.batch_hours
    )
    
    # ä¸‹è¼‰æ•¸æ“š
    df = downloader.download_range(
        start_date=args.start,
        end_date=args.end,
        resume=not args.no_resume
    )
    
    # åˆä½µåˆ° Kç·š
    if not args.no_merge and not df.empty:
        downloader.merge_with_klines()
    
    print("="*70)
    print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    print("="*70)


if __name__ == "__main__":
    main()
