"""
åˆä½µæ‰¹æ¬¡æª”æ¡ˆä¸¦å®Œæˆä¸‹è¼‰
"""

import sys
from pathlib import Path
import pandas as pd
import json
from datetime import datetime

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def merge_batches_for_interval(data_dir: Path, symbol: str, interval: str):
    """åˆä½µç‰¹å®šæ™‚é–“æ¡†æ¶çš„æ‰¹æ¬¡æª”æ¡ˆ"""
    batch_files = sorted(data_dir.glob(f"{symbol}_{interval}_batch_*.parquet"))
    
    if not batch_files:
        print(f"âŒ æ²’æœ‰æ‰¾åˆ° {interval} çš„æ‰¹æ¬¡æª”æ¡ˆ")
        return None
    
    print(f"\nğŸ“¦ åˆä½µ {interval} æ‰¹æ¬¡æª”æ¡ˆ: {len(batch_files)} å€‹")
    
    dfs = []
    for batch_file in batch_files:
        try:
            df = pd.read_parquet(batch_file)
            dfs.append(df)
        except Exception as e:
            print(f"   âš ï¸  è®€å–å¤±æ•—: {batch_file.name} - {e}")
            continue
    
    if not dfs:
        return None
    
    # åˆä½µä¸¦å»é‡
    print(f"   åˆä½µ {len(dfs)} å€‹æ‰¹æ¬¡...")
    df = pd.concat(dfs, ignore_index=True)
    
    print(f"   å»é‡...")
    before_dedup = len(df)
    df = df.drop_duplicates(subset=['timestamp']).sort_values('timestamp').reset_index(drop=True)
    after_dedup = len(df)
    
    if before_dedup != after_dedup:
        print(f"   å»é™¤ {before_dedup - after_dedup:,} ç­†é‡è¤‡è³‡æ–™")
    
    # å„²å­˜æœ€çµ‚æª”æ¡ˆ
    final_file = data_dir / f"{symbol}_{interval}.parquet"
    df.to_parquet(final_file, index=False)
    file_size_mb = final_file.stat().st_size / 1024 / 1024
    
    print(f"   âœ… å·²å„²å­˜: {final_file.name}")
    print(f"      Kç·šæ•¸: {len(df):,}")
    print(f"      å¤§å°: {file_size_mb:.2f} MB")
    print(f"      æ™‚é–“ç¯„åœ: {df.iloc[0]['timestamp']} ~ {df.iloc[-1]['timestamp']}")
    
    # åˆªé™¤æ‰¹æ¬¡æª”æ¡ˆ
    print(f"   ğŸ—‘ï¸  åˆªé™¤ {len(batch_files)} å€‹æ‰¹æ¬¡æª”æ¡ˆ...")
    for batch_file in batch_files:
        batch_file.unlink()
    
    return {
        'rows': len(df),
        'size_mb': round(file_size_mb, 2),
        'start': df.iloc[0]['timestamp'].isoformat(),
        'end': df.iloc[-1]['timestamp'].isoformat(),
    }


def main():
    """ä¸»å‡½æ•¸"""
    print("\nğŸ”„ åˆä½µæ‰¹æ¬¡æª”æ¡ˆ\n")
    print("=" * 60)
    
    data_dir = Path("data/historical")
    symbol = "BTCUSDT"
    
    # æª¢æ¸¬æœ‰å“ªäº›æ™‚é–“æ¡†æ¶çš„æ‰¹æ¬¡æª”æ¡ˆ
    all_batch_files = list(data_dir.glob(f"{symbol}_*_batch_*.parquet"))
    
    if not all_batch_files:
        print("âŒ æ²’æœ‰æ‰¾åˆ°æ‰¹æ¬¡æª”æ¡ˆ")
        return
    
    # æå–æ™‚é–“æ¡†æ¶
    intervals = set()
    for file in all_batch_files:
        # BTCUSDT_1m_batch_0.parquet -> 1m
        parts = file.stem.split('_')
        if len(parts) >= 3:
            intervals.add(parts[1])
    
    intervals = sorted(intervals)
    print(f"æ‰¾åˆ°æ™‚é–“æ¡†æ¶: {', '.join(intervals)}\n")
    
    # çµ±è¨ˆè³‡è¨Š
    stats = {
        'intervals': {},
        'total_rows': 0,
        'total_size_mb': 0,
        'start_time': datetime.now().isoformat(),
    }
    
    # é€å€‹åˆä½µ
    for interval in intervals:
        try:
            result = merge_batches_for_interval(data_dir, symbol, interval)
            if result:
                stats['intervals'][interval] = result
                stats['total_rows'] += result['rows']
                stats['total_size_mb'] += result['size_mb']
        except Exception as e:
            print(f"âŒ {interval} åˆä½µå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    stats['end_time'] = datetime.now().isoformat()
    stats['duration_minutes'] = round(
        (datetime.fromisoformat(stats['end_time']) - 
         datetime.fromisoformat(stats['start_time'])).total_seconds() / 60,
        2
    )
    
    # å„²å­˜çµ±è¨ˆè³‡è¨Š
    stats_file = data_dir / 'download_stats.json'
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    # é¡¯ç¤ºç¸½çµ
    print("\n" + "=" * 60)
    print("âœ… åˆä½µå®Œæˆï¼")
    print("=" * 60)
    print()
    print("ğŸ“Š çµ±è¨ˆè³‡è¨Š:")
    print(f"   ç¸½Kç·šæ•¸: {stats['total_rows']:,} rows")
    print(f"   ç¸½å¤§å°: {stats['total_size_mb']:.2f} MB")
    print(f"   è€—æ™‚: {stats['duration_minutes']:.2f} åˆ†é˜")
    print()
    print("ğŸ“ å„æ™‚é–“æ¡†æ¶:")
    for interval, info in stats['intervals'].items():
        days = (pd.to_datetime(info['end']) - pd.to_datetime(info['start'])).days
        print(f"   {interval:>4s}: {info['rows']:>10,} rows, {info['size_mb']:>8.2f} MB, {days:>4} å¤©")
    print()
    print(f"ğŸ’¾ è³‡æ–™ç›®éŒ„: {data_dir.absolute()}")
    print(f"ğŸ“„ çµ±è¨ˆæª”æ¡ˆ: {stats_file.absolute()}")
    print()


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ä½¿ç”¨è€…ä¸­æ–·")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
