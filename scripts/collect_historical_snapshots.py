"""
çœŸå¯¦æ­·å²æ•¸æ“šæ”¶é›†å™¨

Purpose:
    æ”¶é›†çœŸå¯¦ Binance WebSocket æ•¸æ“šï¼ˆorderbook + tradesï¼‰
    ä¿å­˜ç‚º Parquet æ ¼å¼ï¼Œç”¨æ–¼æœªä¾†çš„æº–ç¢ºå›æ¸¬

Output:
    - BTCUSDT_orderbook_YYYYMMDD.parquet
    - BTCUSDT_trades_YYYYMMDD.parquet
"""

import sys
import asyncio
from binance import AsyncClient, BinanceSocketManager
import pandas as pd
from datetime import datetime
import json
import time
import os


class HistoricalDataCollector:

import asyncio
import json
from datetime import datetime
from pathlib import Path
import pandas as pd
from typing import List, Dict
import logging

from binance import AsyncClient
from binance.streams import BinanceSocketManager

logger = logging.getLogger(__name__)


class HistoricalDataCollector:
    """
    æ­·å²æ•¸æ“šæ”¶é›†å™¨
    
    å¯¦æ™‚æ”¶é›†ä¸¦å­˜å„² Binance å¸‚å ´æ•¸æ“šï¼Œä¾›æœªä¾†å›æ¸¬ä½¿ç”¨
    """
    
    def __init__(
        self,
        symbol: str = "BTCUSDT",
        save_dir: str = "data/historical_snapshots"
    ):
        self.symbol = symbol
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # æ•¸æ“šç·©è¡å€
        self.orderbook_buffer: List[Dict] = []
        self.trade_buffer: List[Dict] = []
        
        # ç·©è¡å€å¤§å°ï¼ˆé”åˆ°æ­¤å¤§å°æ™‚å¯«å…¥ç£ç›¤ï¼‰
        self.buffer_size = 1000
        
        # ç•¶å‰æ—¥æœŸï¼ˆç”¨æ–¼æ–‡ä»¶å‘½åï¼‰
        self.current_date = datetime.now().strftime("%Y%m%d")
        
        logger.info(f"åˆå§‹åŒ– HistoricalDataCollector: {symbol}")
        logger.info(f"å­˜å„²ç›®éŒ„: {self.save_dir}")
    
    def _get_filename(self, data_type: str) -> Path:
        """ç²å–æ–‡ä»¶å"""
        return self.save_dir / f"{self.symbol}_{data_type}_{self.current_date}.parquet"
    
    def process_orderbook(self, msg: Dict):
        """è™•ç†è¨‚å–®ç°¿æ¶ˆæ¯"""
        try:
            timestamp = msg['E']  # Event time
            
            # æå–å‰ 20 æª”
            bids = msg['b'][:20]  # [[price, qty], ...]
            asks = msg['a'][:20]
            
            # æ§‹å»ºè¨˜éŒ„
            record = {
                'timestamp': timestamp,
                'event_time': datetime.fromtimestamp(timestamp / 1000),
                'bids': json.dumps(bids),  # åºåˆ—åŒ–ç‚º JSON å­—ç¬¦ä¸²
                'asks': json.dumps(asks),
            }
            
            self.orderbook_buffer.append(record)
            
            # å¦‚æœç·©è¡å€æ»¿äº†ï¼Œå¯«å…¥ç£ç›¤
            if len(self.orderbook_buffer) >= self.buffer_size:
                self._flush_orderbook()
        
        except Exception as e:
            logger.error(f"è™•ç†è¨‚å–®ç°¿éŒ¯èª¤: {e}")
    
    def process_trade(self, msg: Dict):
        """è™•ç†äº¤æ˜“æ¶ˆæ¯"""
        try:
            # æ§‹å»ºè¨˜éŒ„
            record = {
                'timestamp': msg['E'],
                'event_time': datetime.fromtimestamp(msg['E'] / 1000),
                'trade_id': msg['a'],
                'price': float(msg['p']),
                'quantity': float(msg['q']),
                'is_buyer_maker': msg['m'],
            }
            
            self.trade_buffer.append(record)
            
            # å¦‚æœç·©è¡å€æ»¿äº†ï¼Œå¯«å…¥ç£ç›¤
            if len(self.trade_buffer) >= self.buffer_size:
                self._flush_trades()
        
        except Exception as e:
            logger.error(f"è™•ç†äº¤æ˜“éŒ¯èª¤: {e}")
    
    def _flush_orderbook(self):
        """å°‡è¨‚å–®ç°¿ç·©è¡å€å¯«å…¥ç£ç›¤"""
        if not self.orderbook_buffer:
            return
        
        filename = self._get_filename("orderbook")
        df = pd.DataFrame(self.orderbook_buffer)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿½åŠ 
        if filename.exists():
            existing_df = pd.read_parquet(filename)
            df = pd.concat([existing_df, df], ignore_index=True)
        
        df.to_parquet(filename, index=False, compression='snappy')
        
        logger.info(f"å¯«å…¥ {len(self.orderbook_buffer)} æ¢è¨‚å–®ç°¿è¨˜éŒ„åˆ° {filename}")
        self.orderbook_buffer = []
    
    def _flush_trades(self):
        """å°‡äº¤æ˜“ç·©è¡å€å¯«å…¥ç£ç›¤"""
        if not self.trade_buffer:
            return
        
        filename = self._get_filename("trades")
        df = pd.DataFrame(self.trade_buffer)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿½åŠ 
        if filename.exists():
            existing_df = pd.read_parquet(filename)
            df = pd.concat([existing_df, df], ignore_index=True)
        
        df.to_parquet(filename, index=False, compression='snappy')
        
        logger.info(f"å¯«å…¥ {len(self.trade_buffer)} æ¢äº¤æ˜“è¨˜éŒ„åˆ° {filename}")
        self.trade_buffer = []
    
    def flush_all(self):
        """å¼·åˆ¶å¯«å…¥æ‰€æœ‰ç·©è¡å€"""
        self._flush_orderbook()
        self._flush_trades()
        logger.info("æ‰€æœ‰ç·©è¡å€å·²å¯«å…¥ç£ç›¤")
    
    async def start_collection(self, duration_hours: int = 24):
        """
        é–‹å§‹æ”¶é›†æ•¸æ“š
        
        Args:
            duration_hours: æ”¶é›†æ™‚é•·ï¼ˆå°æ™‚ï¼‰
        """
        logger.info(f"é–‹å§‹æ”¶é›†æ•¸æ“šï¼Œé è¨ˆé‹è¡Œ {duration_hours} å°æ™‚")
        
        client = await AsyncClient.create()
        bsm = BinanceSocketManager(client)
        
        # è¨‚å–®ç°¿ WebSocket
        orderbook_socket = bsm.depth_socket(self.symbol, depth=BinanceSocketManager.WEBSOCKET_DEPTH_20)
        
        # äº¤æ˜“ WebSocket
        trade_socket = bsm.aggtrade_socket(self.symbol)
        
        try:
            async with orderbook_socket as ob_stream, trade_socket as trade_stream:
                end_time = datetime.now().timestamp() + duration_hours * 3600
                
                while datetime.now().timestamp() < end_time:
                    # æ¥æ”¶è¨‚å–®ç°¿æ›´æ–°
                    ob_msg = await asyncio.wait_for(ob_stream.recv(), timeout=1.0)
                    self.process_orderbook(ob_msg)
                    
                    # æ¥æ”¶äº¤æ˜“
                    try:
                        trade_msg = await asyncio.wait_for(trade_stream.recv(), timeout=0.1)
                        self.process_trade(trade_msg)
                    except asyncio.TimeoutError:
                        pass
                    
                    # æ¯ 60 ç§’æª¢æŸ¥æ˜¯å¦éœ€è¦æ–°å»ºæ–‡ä»¶ï¼ˆæ–°çš„ä¸€å¤©ï¼‰
                    current_date = datetime.now().strftime("%Y%m%d")
                    if current_date != self.current_date:
                        self.flush_all()
                        self.current_date = current_date
                        logger.info(f"æ—¥æœŸè®Šæ›´ï¼Œæ–°æ–‡ä»¶: {current_date}")
        
        except Exception as e:
            logger.error(f"æ”¶é›†æ•¸æ“šéŒ¯èª¤: {e}")
        
        finally:
            # ç¢ºä¿æ‰€æœ‰æ•¸æ“šéƒ½å¯«å…¥
            self.flush_all()
            await client.close_connection()
            logger.info("æ•¸æ“šæ”¶é›†å·²åœæ­¢")


async def main():
    """æ”¶é›†æ•¸æ“š"""
    import sys
    
    # å‘½ä»¤è¡Œåƒæ•¸
    duration_hours = float(sys.argv[1]) if len(sys.argv) > 1 else 24
    save_dir = sys.argv[2] if len(sys.argv) > 2 else "data/historical_snapshots"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    print(f"ğŸ“¥ é–‹å§‹æ”¶é›†çœŸå¯¦å¸‚å ´æ•¸æ“š")
    print(f"   æ™‚é•·: {duration_hours} å°æ™‚")
    print(f"   ä¿å­˜: {save_dir}")
    print()
    
    collector = HistoricalDataCollector(
        symbol="BTCUSDT",
        save_dir=save_dir
    )
    
    await collector.start_collection(duration_hours=duration_hours)


if __name__ == "__main__":
    asyncio.run(main())
