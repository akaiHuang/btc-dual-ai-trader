#!/usr/bin/env python3
"""æ‰¹æ¬¡ä¸‹è¼‰ Binance Data Portal çš„ Futures Depth / Book Snapshot æª”æ¡ˆã€‚

ç”¨æ³•ç¯„ä¾‹ï¼š
    python scripts/download_binance_depth.py \
        --symbol BTCUSDT \
        --start 2020-11-11 \
        --end 2020-11-30 \
        --dataset depth \
        --output data/historical/depth_raw

è³‡æ–™ä¾†æºèªªæ˜ï¼š
- å®˜æ–¹å…¥å£: https://data.binance.vision/
- Futures (USDâ“ˆ-M / COIN-M) æª”æ¡ˆè·¯å¾‘æ ¼å¼ï¼š
  data/futures/{market}/{frequency}/{dataset}/{symbol}/{file_name}.zip
  å…¶ä¸­ frequency å¯ç‚º daily/weekly/monthlyï¼Œç›®å‰è…³æœ¬ä»¥ daily ç‚ºä¸»ã€‚

æ³¨æ„ï¼š
- å®˜æ–¹ zip æª”é€šå¸¸å¾ˆå¤§ï¼Œä¸‹è¼‰å‰è«‹ç¢ºä¿ç£ç¢Ÿç©ºé–“å……è¶³ã€‚
- æª”æ¡ˆè‹¥ä¸å­˜åœ¨æœƒæ”¶åˆ° 404ï¼Œè…³æœ¬æœƒè¨˜éŒ„ miss æ¸…å–®ä¾›å¾ŒçºŒé‡è©¦ã€‚
- ä¸‹è¼‰å¾Œé è¨­åŒæ™‚ä¿ç•™ zip èˆ‡è§£å£“å‡ºçš„ csvï¼Œå¯ç”¨ --no-keep-zip åƒ…ä¿ç•™ csvã€‚
"""

from __future__ import annotations

import argparse
import datetime as dt
import os
from pathlib import Path
from typing import Iterable, List, Tuple
import zipfile

import requests

BASE_URL = "https://data.binance.vision"
DATASET_FOLDER = {
    "depth": "depth",
    "book": "bookDepth",
    "diff": "bookDepthSnapshot"  # å·®åˆ†æª” (è‹¥å®˜æ–¹æä¾›)
}


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Download Binance Futures depth/book snapshots")
    parser.add_argument("--symbol", default="BTCUSDT", help="äº¤æ˜“å° (é è¨­: BTCUSDT)")
    parser.add_argument("--market", choices=["um", "cm"], default="um", help="um=USDâ“ˆ-M, cm=COIN-M")
    parser.add_argument("--dataset", choices=["depth", "book", "diff"], default="depth",
                        help="ä¸‹è¼‰å“ªç¨® dataset: depth(é€ç­† diff)ã€book(å®Œæ•´å¿«ç…§)ã€diff(å®˜æ–¹å·®åˆ†)")
    parser.add_argument("--start", required=True, help="é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--end", required=True, help="çµæŸæ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--output", default="data/historical/depth_raw",
                        help="è¼¸å‡ºè³‡æ–™å¤¾ (é è¨­: data/historical/depth_raw)")
    parser.add_argument("--frequency", choices=["daily", "monthly"], default="daily",
                        help="å®˜æ–¹æª”æ¡ˆé »ç‡ (é è¨­ daily)")
    parser.add_argument("--keep-zip", action="store_true", help="ä¿ç•™ä¸‹è¼‰çš„ zip (é è¨­æœƒåˆªé™¤)")
    parser.add_argument("--skip-existing", action="store_true", help="è‹¥ csv å·²å­˜åœ¨å‰‡è·³éä¸‹è¼‰")
    parser.add_argument("--timeout", type=int, default=60, help="HTTP é€¾æ™‚ç§’æ•¸ (é è¨­ 60)")
    parser.add_argument("--max-retries", type=int, default=3, help="å–®æª”æ¡ˆä¸‹è¼‰æœ€å¤§é‡è©¦æ¬¡æ•¸")
    return parser.parse_args()


def iter_dates(start: dt.date, end: dt.date) -> Iterable[dt.date]:
    cur = start
    while cur <= end:
        yield cur
        cur += dt.timedelta(days=1)


def build_daily_path(symbol: str, dataset: str, market: str, target_date: dt.date) -> Tuple[str, str]:
    folder = DATASET_FOLDER[dataset]
    date_str = target_date.strftime("%Y-%m-%d")
    file_name = f"{symbol}-{folder}-{date_str}.zip"
    url_path = f"data/futures/{market}/daily/{folder}/{symbol}/{file_name}"
    return url_path, file_name


def ensure_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def download_file(session: requests.Session, url: str, dest: Path, timeout: int, retries: int) -> bool:
    for attempt in range(1, retries + 1):
        try:
            with session.get(url, timeout=timeout, stream=True) as resp:
                if resp.status_code == 404:
                    return False
                resp.raise_for_status()
                with open(dest, "wb") as fh:
                    for chunk in resp.iter_content(chunk_size=1 << 16):
                        if chunk:
                            fh.write(chunk)
            return True
        except requests.RequestException as exc:
            print(f"âš ï¸  ä¸‹è¼‰å¤±æ•— {url} (attempt {attempt}/{retries}): {exc}")
    return False


def extract_zip(zip_path: Path, output_dir: Path) -> List[str]:
    extracted = []
    with zipfile.ZipFile(zip_path) as zf:
        for member in zf.namelist():
            target_path = output_dir / member
            zf.extract(member, output_dir)
            extracted.append(str(target_path))
    return extracted


def main() -> None:
    args = parse_args()
    start_date = dt.datetime.strptime(args.start, "%Y-%m-%d").date()
    end_date = dt.datetime.strptime(args.end, "%Y-%m-%d").date()
    if start_date > end_date:
        raise SystemExit("start date must be earlier than end date")

    output_dir = Path(args.output)
    ensure_dir(output_dir)
    print(f"ğŸ“ ä¸‹è¼‰è¼¸å‡ºè³‡æ–™å¤¾: {output_dir.resolve()}")

    downloads = 0
    misses = []
    skipped = 0

    with requests.Session() as session:
        for cur_date in iter_dates(start_date, end_date):
            url_path, file_name = build_daily_path(args.symbol.upper(), args.dataset, args.market, cur_date)
            url = f"{BASE_URL}/{url_path}"
            csv_name = file_name.replace(".zip", ".csv")
            csv_path = output_dir / csv_name
            zip_path = output_dir / file_name

            if args.skip_existing and csv_path.exists():
                print(f"â© å·²å­˜åœ¨ï¼Œè·³é: {csv_name}")
                skipped += 1
                continue

            print(f"â¬‡ï¸  ä¸‹è¼‰ {url}")
            ok = download_file(session, url, zip_path, args.timeout, args.max_retries)
            if not ok:
                print(f"âŒ æ‰¾ä¸åˆ°æˆ–ä¸‹è¼‰å¤±æ•—: {url}")
                misses.append(url)
                continue

            extracted_files = extract_zip(zip_path, output_dir)
            print(f"âœ… è§£å£“å®Œæˆ: {', '.join(Path(f).name for f in extracted_files)}")
            downloads += 1

            if not args.keep_zip:
                zip_path.unlink(missing_ok=True)

    print("\n===== æ‘˜è¦ =====")
    print(f"æˆåŠŸä¸‹è¼‰: {downloads} å¤©")
    print(f"è·³é (å·²æœ‰æª”æ¡ˆ): {skipped} å¤©")
    if misses:
        print(f"ç¼ºå¤±/å¤±æ•—: {len(misses)} å¤©ï¼Œæ¸…å–®å¦‚ä¸‹ï¼š")
        for url in misses:
            print(f"  - {url}")
        miss_log = output_dir / "missing_depth_urls.txt"
        with open(miss_log, "w") as fh:
            for url in misses:
                fh.write(url + "\n")
        print(f"å·²å°‡ç¼ºå¤±åˆ—è¡¨å¯«å…¥: {miss_log}")
    else:
        print("ç„¡ç¼ºå¤±æª”æ¡ˆ ğŸ‰")


if __name__ == "__main__":
    main()
