"""
æ­·å²è³‡æ–™ä¸‹è¼‰è…³æœ¬
ä¸‹è¼‰ 5 å¹´ BTC/USDT Kç·šè³‡æ–™ä¸¦å„²å­˜åˆ°æœ¬åœ°

æ™‚é–“æ¡†æ¶ï¼š1m, 3m, 15m, 1h, 1d, 1w
é ä¼°è³‡æ–™é‡ï¼š~50GB
"""

import sys
from pathlib import Path
from datetime import datetime, timedelta
import time
import pandas as pd
from tqdm import tqdm
import json

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.exchange.binance_client import BinanceClient


class HistoricalDataDownloader:
    """æ­·å²è³‡æ–™ä¸‹è¼‰å™¨"""
    
    # Binance Kç·šè³‡æ–™é™åˆ¶
    MAX_KLINES_PER_REQUEST = 1000
    
    # æ™‚é–“æ¡†æ¶èˆ‡æ¯«ç§’å°æ‡‰
    INTERVAL_MS = {
        '1m': 60 * 1000,
        '3m': 3 * 60 * 1000,
        '5m': 5 * 60 * 1000,
        '15m': 15 * 60 * 1000,
        '30m': 30 * 60 * 1000,
        '1h': 60 * 60 * 1000,
        '2h': 2 * 60 * 60 * 1000,
        '4h': 4 * 60 * 60 * 1000,
        '6h': 6 * 60 * 60 * 1000,
        '12h': 12 * 60 * 60 * 1000,
        '1d': 24 * 60 * 60 * 1000,
        '1w': 7 * 24 * 60 * 60 * 1000,
    }
    
    def __init__(self, symbol: str = "BTCUSDT", data_dir: str = "data/historical", use_mainnet: bool = True):
        """
        åˆå§‹åŒ–ä¸‹è¼‰å™¨
        
        Args:
            symbol: äº¤æ˜“å°ç¬¦è™Ÿ
            data_dir: è³‡æ–™å„²å­˜ç›®éŒ„
            use_mainnet: æ˜¯å¦ä½¿ç”¨ Mainnetï¼ˆTrue=æ­£å¼ç’°å¢ƒï¼ŒFalse=æ¸¬è©¦ç¶²ï¼‰
        """
        self.client = BinanceClient()
        
        # å¦‚æœä½¿ç”¨ Mainnetï¼Œç›´æ¥è¦†è“‹ API URLï¼ˆç„¡éœ€ API Key å³å¯å–å¾—å¸‚å ´è³‡æ–™ï¼‰
        if use_mainnet:
            from binance.client import Client
            self.client.client = Client(api_key="", api_secret="")  # å…¬é–‹è³‡æ–™ç„¡éœ€ API Key
            print(f"âœ… åˆå§‹åŒ–ä¸‹è¼‰å™¨ (Mainnet - æ­£å¼ç’°å¢ƒ)")
        else:
            print(f"âœ… åˆå§‹åŒ–ä¸‹è¼‰å™¨ (Testnet - æ¸¬è©¦ç¶²)")
        
        self.symbol = symbol
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"   äº¤æ˜“å°: {symbol}")
        print(f"   è³‡æ–™ç›®éŒ„: {self.data_dir.absolute()}")
        print()
    
    def calculate_required_requests(self, interval: str, years: int) -> int:
        """
        è¨ˆç®—éœ€è¦çš„è«‹æ±‚æ¬¡æ•¸
        
        Args:
            interval: æ™‚é–“æ¡†æ¶
            years: å¹´æ•¸
            
        Returns:
            è«‹æ±‚æ¬¡æ•¸
        """
        interval_ms = self.INTERVAL_MS[interval]
        total_ms = years * 365 * 24 * 60 * 60 * 1000
        total_klines = total_ms // interval_ms
        requests = (total_klines // self.MAX_KLINES_PER_REQUEST) + 1
        return requests
    
    def download_klines(
        self,
        interval: str,
        start_time: datetime,
        end_time: datetime,
        save_batch_size: int = 10000
    ) -> pd.DataFrame:
        """
        ä¸‹è¼‰æŒ‡å®šæ™‚é–“ç¯„åœçš„ Kç·šè³‡æ–™
        
        Args:
            interval: æ™‚é–“æ¡†æ¶
            start_time: é–‹å§‹æ™‚é–“
            end_time: çµæŸæ™‚é–“
            save_batch_size: æ‰¹æ¬¡å„²å­˜å¤§å°
            
        Returns:
            Kç·šè³‡æ–™ DataFrame
        """
        print(f"ğŸ“¥ ä¸‹è¼‰ {interval} Kç·šè³‡æ–™")
        print(f"   æ™‚é–“ç¯„åœ: {start_time.date()} ~ {end_time.date()}")
        
        # è¨ˆç®—é ä¼°è«‹æ±‚æ¬¡æ•¸
        days = (end_time - start_time).days
        estimated_requests = self.calculate_required_requests(interval, days / 365)
        print(f"   é ä¼°è«‹æ±‚: ~{estimated_requests} æ¬¡")
        
        # æº–å‚™è³‡æ–™å®¹å™¨
        all_klines = []
        current_start = int(start_time.timestamp() * 1000)
        end_ts = int(end_time.timestamp() * 1000)
        
        # é€²åº¦æ¢
        pbar = tqdm(total=estimated_requests, desc=f"  {interval}", unit="req")
        
        batch_count = 0
        while current_start < end_ts:
            try:
                # ä¸‹è¼‰ä¸€æ‰¹è³‡æ–™
                klines = self.client.get_klines(
                    symbol=self.symbol,
                    interval=interval,
                    start_time=current_start,
                    limit=self.MAX_KLINES_PER_REQUEST
                )
                
                if not klines:
                    break
                
                # æ·»åŠ åˆ°åˆ—è¡¨
                all_klines.extend(klines)
                
                # æ›´æ–°èµ·å§‹æ™‚é–“
                current_start = int(klines[-1][6]) + 1  # æœ€å¾Œä¸€æ ¹Kç·šçš„æ”¶ç›¤æ™‚é–“ + 1ms
                
                # æ›´æ–°é€²åº¦æ¢
                pbar.update(1)
                
                # æ‰¹æ¬¡å„²å­˜ï¼ˆé¿å…è¨˜æ†¶é«”æº¢å‡ºï¼‰
                if len(all_klines) >= save_batch_size:
                    self._save_batch(all_klines, interval, batch_count)
                    batch_count += 1
                    all_klines = []
                
                # é¿å…è§¸ç™¼é€Ÿç‡é™åˆ¶
                time.sleep(0.1)
                
            except Exception as e:
                print(f"\nâŒ ä¸‹è¼‰éŒ¯èª¤: {e}")
                print(f"   ç•¶å‰æ™‚é–“æˆ³: {current_start}")
                time.sleep(5)
                continue
        
        pbar.close()
        
        # å„²å­˜å‰©é¤˜è³‡æ–™
        if all_klines:
            self._save_batch(all_klines, interval, batch_count)
        
        # åˆä½µæ‰€æœ‰æ‰¹æ¬¡
        df = self._merge_batches(interval, batch_count + 1)
        
        print(f"   âœ… å®Œæˆï¼ç¸½å…± {len(df):,} æ ¹Kç·š")
        print(f"   æ™‚é–“ç¯„åœ: {df.iloc[0]['timestamp']} ~ {df.iloc[-1]['timestamp']}")
        print()
        
        return df
    
    def _save_batch(self, klines: list, interval: str, batch_num: int):
        """å„²å­˜æ‰¹æ¬¡è³‡æ–™"""
        df = self._klines_to_dataframe(klines)
        batch_file = self.data_dir / f"{self.symbol}_{interval}_batch_{batch_num}.parquet"
        df.to_parquet(batch_file, index=False)
    
    def _merge_batches(self, interval: str, num_batches: int) -> pd.DataFrame:
        """åˆä½µæ‰€æœ‰æ‰¹æ¬¡ä¸¦åˆªé™¤è‡¨æ™‚æª”æ¡ˆ"""
        dfs = []
        
        for i in range(num_batches):
            batch_file = self.data_dir / f"{self.symbol}_{interval}_batch_{i}.parquet"
            if batch_file.exists():
                dfs.append(pd.read_parquet(batch_file))
                batch_file.unlink()  # åˆªé™¤è‡¨æ™‚æª”æ¡ˆ
        
        if not dfs:
            return pd.DataFrame()
        
        # åˆä½µä¸¦å»é‡
        df = pd.concat(dfs, ignore_index=True)
        df = df.drop_duplicates(subset=['timestamp']).sort_values('timestamp').reset_index(drop=True)
        
        # å„²å­˜æœ€çµ‚æª”æ¡ˆ
        final_file = self.data_dir / f"{self.symbol}_{interval}.parquet"
        df.to_parquet(final_file, index=False)
        print(f"   ğŸ’¾ å·²å„²å­˜: {final_file.name} ({len(df):,} rows, {final_file.stat().st_size / 1024 / 1024:.2f} MB)")
        
        return df
    
    def _klines_to_dataframe(self, klines: list) -> pd.DataFrame:
        """
        å°‡ Kç·šè³‡æ–™è½‰æ›ç‚º DataFrame
        
        Args:
            klines: Kç·šè³‡æ–™åˆ—è¡¨
            
        Returns:
            DataFrame
        """
        df = pd.DataFrame(klines, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_volume', 'trades', 'taker_buy_base',
            'taker_buy_quote', 'ignore'
        ])
        
        # è½‰æ›è³‡æ–™é¡å‹
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df['close_time'] = pd.to_datetime(df['close_time'], unit='ms')
        
        for col in ['open', 'high', 'low', 'close', 'volume', 'quote_volume', 'taker_buy_base', 'taker_buy_quote']:
            df[col] = df[col].astype(float)
        
        df['trades'] = df['trades'].astype(int)
        
        # åˆªé™¤ä¸éœ€è¦çš„æ¬„ä½
        df = df.drop(columns=['ignore'])
        
        return df
    
    def download_all_intervals(
        self,
        intervals: list = ['1m', '3m', '15m', '1h', '1d', '1w'],
        years: int = 5
    ):
        """
        ä¸‹è¼‰æ‰€æœ‰æ™‚é–“æ¡†æ¶çš„è³‡æ–™
        
        Args:
            intervals: æ™‚é–“æ¡†æ¶åˆ—è¡¨
            years: ä¸‹è¼‰å¹´æ•¸
        """
        print("=" * 60)
        print("ğŸ“Š BTC/USDT æ­·å²è³‡æ–™ä¸‹è¼‰")
        print("=" * 60)
        print()
        
        end_time = datetime.now()
        start_time = end_time - timedelta(days=years * 365)
        
        print(f"æ™‚é–“ç¯„åœ: {start_time.date()} ~ {end_time.date()}")
        print(f"æ™‚é–“æ¡†æ¶: {', '.join(intervals)}")
        print(f"äº¤æ˜“å°: {self.symbol}")
        print()
        
        # çµ±è¨ˆè³‡è¨Š
        stats = {
            'intervals': {},
            'total_rows': 0,
            'total_size_mb': 0,
            'start_time': datetime.now().isoformat(),
        }
        
        # é€å€‹ä¸‹è¼‰
        for interval in intervals:
            try:
                df = self.download_klines(interval, start_time, end_time)
                
                file_path = self.data_dir / f"{self.symbol}_{interval}.parquet"
                file_size_mb = file_path.stat().st_size / 1024 / 1024
                
                stats['intervals'][interval] = {
                    'rows': len(df),
                    'size_mb': round(file_size_mb, 2),
                    'start': df.iloc[0]['timestamp'].isoformat(),
                    'end': df.iloc[-1]['timestamp'].isoformat(),
                }
                
                stats['total_rows'] += len(df)
                stats['total_size_mb'] += file_size_mb
                
            except Exception as e:
                print(f"âŒ {interval} ä¸‹è¼‰å¤±æ•—: {e}")
                continue
        
        stats['end_time'] = datetime.now().isoformat()
        stats['duration_minutes'] = round(
            (datetime.fromisoformat(stats['end_time']) - 
             datetime.fromisoformat(stats['start_time'])).total_seconds() / 60,
            2
        )
        
        # å„²å­˜çµ±è¨ˆè³‡è¨Š
        stats_file = self.data_dir / 'download_stats.json'
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        # é¡¯ç¤ºç¸½çµ
        print("=" * 60)
        print("âœ… ä¸‹è¼‰å®Œæˆï¼")
        print("=" * 60)
        print()
        print("ğŸ“Š çµ±è¨ˆè³‡è¨Š:")
        print(f"   ç¸½Kç·šæ•¸: {stats['total_rows']:,} rows")
        print(f"   ç¸½å¤§å°: {stats['total_size_mb']:.2f} MB")
        print(f"   è€—æ™‚: {stats['duration_minutes']:.2f} åˆ†é˜")
        print()
        print("ğŸ“ å„æ™‚é–“æ¡†æ¶:")
        for interval, info in stats['intervals'].items():
            print(f"   {interval:>4s}: {info['rows']:>10,} rows, {info['size_mb']:>8.2f} MB")
        print()
        print(f"ğŸ’¾ è³‡æ–™ç›®éŒ„: {self.data_dir.absolute()}")
        print(f"ğŸ“„ çµ±è¨ˆæª”æ¡ˆ: {stats_file.absolute()}")
        print()


def main():
    """ä¸»å‡½æ•¸"""
    print("\nğŸš€ Task 1.3: æ­·å²è³‡æ–™æ”¶é›†\n")
    
    # å‰µå»ºä¸‹è¼‰å™¨ï¼ˆä½¿ç”¨ Mainnet ç²å–å®Œæ•´æ­·å²è³‡æ–™ï¼‰
    downloader = HistoricalDataDownloader(
        symbol="BTCUSDT",
        data_dir="data/historical",
        use_mainnet=True  # ä½¿ç”¨æ­£å¼ç’°å¢ƒç²å–å®Œæ•´è³‡æ–™
    )
    
    # ä¸‹è¼‰è³‡æ–™ï¼ˆ5å¹´ï¼‰
    intervals = ['1m', '3m', '15m', '1h', '1d', '1w']
    downloader.download_all_intervals(intervals=intervals, years=5)
    
    print("ğŸ¯ Task 1.3 å®Œæˆï¼")


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ä½¿ç”¨è€…ä¸­æ–·ä¸‹è¼‰")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
